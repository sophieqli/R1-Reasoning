# -*- coding: utf-8 -*-
"""LLM finetuning with HugginFace Peft Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/142ZefZIlCcbxt2eTSqXFt9Z6BL1o4TME

# LLM finetuning with HuggingFace Peft Model
"""

#!pip install transformers
#!pip install datasets
#!pip uninstall -y torch torchvision torchaudio
#!pip install torch==2.2.0+cu118 torchvision==0.17.0+cu118 torchaudio==2.2.0 --extra-index-url https://download.pytorch.org/whl/cu118
!pip install trl

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from trl import SFTConfig, SFTTrainer
from peft import LoraConfig, TaskType

import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

# load smolLM from hugging face (only ~100M params)
# note you will want to set your colab runtime to use the gpu
MODEL_NAME = "HuggingFaceTB/SmolLM2-135M"
device = "cuda" if torch.cuda.is_available() else "cpu" # set runtime to T4 GPU
print(f"Using device: {device}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)

"""#Printing Model Details (for my own clarity)"""

model

test_prompt = f"### Prompt: Compose a poem, about blue skys \n ### Response:"
inputs = tokenizer.encode(test_prompt, return_tensors="pt").to(device)
outputs = model.generate(inputs, max_length=50, temperature=0.2, top_k=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)
print(outputs[0])

# lets load a dataset with instruction output pairs from hugging face
dataset = load_dataset("checkai/instruction-poems", split="train[:30%]")

from random import randint

#making a random sample
print("col names ", dataset.column_names, "; tot # of samples ", len(dataset))
i = randint(0, len(dataset) - 1)
print("--- Sample ", i, "---")
print("instr:", dataset[i]['INSTRUCTION'])
print("resp:", dataset[i]['RESPONSE'])

# make a peft config

peft_config = LoraConfig(r = 8, lora_alpha = 32, lora_dropout = 0.05, task_type = 'CAUSAL_LM')
from peft import get_peft_model

model = get_peft_model(model, peft_config)
def num_trainable_parameters(model):
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())

    print(f"Trainable parameters: {trainable_params}")
    print(f"All parameters: {total_params}")

num_trainable_parameters(model)

# now we need a function that formats our data into instruction and response
def formatting_func(example):
    text = f"### Prompt: {example['INSTRUCTION']}\n ### Response: {example['RESPONSE']}"
    return text

# create our SFTTrainer
#NOTE: I passed optimizer, lr_scheduler, and gradient accumulation to the trainer (tweaking more hyper-parameters for better training)

sft_config = SFTConfig(
    max_seq_length=100,
    per_device_train_batch_size=4,
    learning_rate=1e-4,
    num_train_epochs=8,
    optim = "adamw_torch",
    lr_scheduler_type="linear",
    #gradient_accumulation_steps=4
)


trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    processing_class=tokenizer,
    formatting_func=formatting_func,
    args=sft_config,
    peft_config=peft_config,
)


# train our model
trainer.train()

instruction = "Write a poem about the sea"
prompt = f"### Prompt: {instruction}\n ### Response: "

input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(input_ids, max_length=50, temperature=0.2, top_k=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)

"""# Calculating the Validation Loss of the Model to check for overfitting"""

#load more samples to validate on
val_dataset = load_dataset("checkai/instruction-poems", split="train[30%:40%]")
#Apply formatting func on the validation data, creating new column "text"
val_dataset = val_dataset.map(lambda x: {"text": formatting_func(x)})

#tokenize w/the same tokenizer settings during training
val_dataset = val_dataset.map(lambda x: tokenizer(x["text"], padding="max_length", truncation=True, max_length=100), batched=True)
print(val_dataset.column_names)

trainer.eval_dataset = val_dataset
metrics = trainer.evaluate()

print("Validation Loss:", metrics["eval_loss"])

"""Seems like the model has a hard time at generalizing to inputs it hasn't seen before, but to be fair the training set is pretty small...

#Input/output length distributions
"""

input_lengths = [len(x["INSTRUCTION"]) for x in val_dataset]
output_lengths = [len(x["RESPONSE"]) for x in val_dataset]
import matplotlib.pyplot as plt
import numpy as np


plt.hist(input_lengths, bins=50)
print("Avg input length: ", np.mean(np.array(input_lengths)))
print("Std. Dev input length: ", np.std(np.array(input_lengths)))
plt.show()

plt.hist(output_lengths, bins=100)
print("Avg output length: ", np.mean(np.array(output_lengths)))
print("Std. Dev input length: ", np.std(np.array(output_lengths)))
plt.xlim(0, 10000)
plt.show()

plt.scatter(input_lengths, output_lengths)
plt.xlabel("Input Length")
plt.ylabel("Output Length")
plt.xlim(0, 450)

plt.ylim(0, 10000)
plt.show()

"""#Most common phrases in the Input

Shows us that the most common prompts are love, life/death, nature, time. These are potential limitations of what the dataset contains, so perhaps it struggles to generalize beyond that.

"""

from collections import Counter
import re

def tokenize(text): return re.findall(r'\w+', text.lower())

instr_tokens = []
for instr in val_dataset["INSTRUCTION"]: instr_tokens += (tokenize(instr))
top_tokens = Counter(instr_tokens).most_common(30)

print("Top 30 words in prompts:")
for word, cnt in top_tokens: print(f"{word}: {cnt}")

