# -*- coding: utf-8 -*-
"""LLM finetuning with HugginFace Peft Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/142ZefZIlCcbxt2eTSqXFt9Z6BL1o4TME

# LLM finetuning with HuggingFace Peft Model
"""

#!pip install transformers
#!pip install datasets
#!pip uninstall -y torch torchvision torchaudio
#!pip install torch==2.2.0+cu118 torchvision==0.17.0+cu118 torchaudio==2.2.0 --extra-index-url https://download.pytorch.org/whl/cu118
!pip install trl

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from trl import SFTConfig, SFTTrainer
from peft import LoraConfig, TaskType

import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

# load smolLM from hugging face (only ~100M params)
# note you will want to set your colab runtime to use the gpu
MODEL_NAME = "HuggingFaceTB/SmolLM2-135M"
device = "cuda" if torch.cuda.is_available() else "cpu" # set runtime to T4 GPU
print(f"Using device: {device}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)

"""#Printing Model Details (for my own clarity)"""

model

# ok lets try prompting it with an instruction, note that it doesn't obey the instruction
# and rather just repeats the format (likely because its never seen this before)
test_prompt = f"### Prompt: Compose a poem, about blue skys \n ### Response:"
inputs = tokenizer.encode(test_prompt, return_tensors="pt").to(device)
outputs = model.generate(inputs, max_length=50, temperature=0.2, top_k=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)
print(outputs[0])

# lets load a dataset with instruction output pairs from hugging face
dataset = load_dataset("checkai/instruction-poems", split="train[:30%]")

from random import randint

#making a random sample
print("col names ", dataset.column_names, "; tot # of samples ", len(dataset))
i = randint(0, len(dataset) - 1)
print("--- Sample ", i, "---")
print("instr:", dataset[i]['INSTRUCTION'])
print("resp:", dataset[i]['RESPONSE'])

# make a peft config

peft_config = LoraConfig(r = 8, lora_alpha = 32, lora_dropout = 0.05, task_type = 'CAUSAL_LM')
from peft import get_peft_model

model = get_peft_model(model, peft_config)
def num_trainable_parameters(model):
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())

    print(f"Trainable parameters: {trainable_params}")
    print(f"All parameters: {total_params}")

num_trainable_parameters(model)

# now we need a function that formats our data into instruction and response
def formatting_func(example):
    text = f"### Prompt: {example['INSTRUCTION']}\n ### Response: {example['RESPONSE']}"
    return text

# create our SFTTrainer
#NOTE: I passed optimizer, lr_scheduler, and gradient accumulation to the trainer (tweaking more hyper-parameters for better training)

sft_config = SFTConfig(
    max_seq_length=100,
    per_device_train_batch_size=4,
    learning_rate=1e-4,
    num_train_epochs=8,
    optim = "adamw_torch",
    lr_scheduler_type="linear",
    #gradient_accumulation_steps=4
)


trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    processing_class=tokenizer,
    formatting_func=formatting_func,
    args=sft_config,
    peft_config=peft_config,
)


# train our model
trainer.train()

instruction = "Write a poem about the sea"
prompt = f"### Prompt: {instruction}\n ### Response: "

input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(input_ids, max_length=50, temperature=0.2, top_k=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)

"""# Calculating the Validation Loss of the Model to check for overfitting"""

#load more samples to validate on
val_dataset = load_dataset("checkai/instruction-poems", split="train[30%:40%]")
#Apply formatting func on the validation data, creating new column "text"
val_dataset = val_dataset.map(lambda x: {"text": formatting_func(x)})

#tokenize w/the same tokenizer settings during training
val_dataset = val_dataset.map(lambda x: tokenizer(x["text"], padding="max_length", truncation=True, max_length=100), batched=True)
print(val_dataset.column_names)

trainer.eval_dataset = val_dataset
metrics = trainer.evaluate()

print("Validation Loss:", metrics["eval_loss"])

"""Seems like the model has a hard time at generalizing to inputs it hasn't seen before, but to be fair the training set is pretty small...

#Input/output length distributions
"""

input_lengths = [len(x["INSTRUCTION"]) for x in val_dataset]
output_lengths = [len(x["RESPONSE"]) for x in val_dataset]
import matplotlib.pyplot as plt
import numpy as np


plt.hist(input_lengths, bins=50)
print("Avg input length: ", np.mean(np.array(input_lengths)))
print("Std. Dev input length: ", np.std(np.array(input_lengths)))
plt.show()

plt.hist(output_lengths, bins=100)
print("Avg output length: ", np.mean(np.array(output_lengths)))
print("Std. Dev input length: ", np.std(np.array(output_lengths)))
plt.xlim(0, 10000)
plt.show()

plt.scatter(input_lengths, output_lengths)
plt.xlabel("Input Length")
plt.ylabel("Output Length")
plt.xlim(0, 450)

plt.ylim(0, 10000)
plt.show()

"""#Most common phrases in the Input

Shows us that the most common prompts are love, life/death, nature, time. These are potential limitations of what the dataset contains, so perhaps it struggles to generalize beyond that.

"""

from collections import Counter
import re

def tokenize(text): return re.findall(r'\w+', text.lower())

instr_tokens = []
for instr in val_dataset["INSTRUCTION"]: instr_tokens += (tokenize(instr))
top_tokens = Counter(instr_tokens).most_common(30)

print("Top 30 words in prompts:")
for word, cnt in top_tokens: print(f"{word}: {cnt}")

"""# Report
Write a short report about what you did and why (we mostly want to check you didn't just use chatgpt for the entire thing).

After importing the libraries and loading the dataset, I began experimenting with the peft model training cell. First, I created the peft_config, which sets the parameters of the model. I noticed that the r = 8 corresponds to the # of trainable parameters (basically the “rank” of the peft model), because when I set r = 4, this number halved.

Then, I initially set up the SFTTrainer’s config with the following parameters: max_seq_length=100, per_device_train_batch_size=4, learning_rate=2e-4, num_train_epochs=3, finishing with a training loss of ~1.83 after ~3000 steps.

Trying to improve the training, I tweaked the learning_rate to 1e-4. I hoped that this would enable the model to take more precise steps, since if the lr is too high, the model might overshoot the optimal weights. To compensate for the smaller updates, I doubled the # of epochs to 6. Hoping to improve stability and convergence, I used the Adam optimizer and a linear LR scheduler.

In this notebook, I printed out the iteration where I used 8 epochs. As one can observe, loss plateaus a bit after 6000 steps in the 1.76-1.78 range, which may suggest a limitation of the dataset itself.

Another tweak I wanted to try was setting Gradient accumulation = 4, which essentially scales up the batch size, so that the gradient of the batch would be closer to the true value. However, this would likely require scaling up epochs by 4 (as the model updates less frequently), and that would be quite computationally slow.  

Extra exploration: I wanted to understand the dataset a bit better, so I printed some extra output: the input/output length distributions and the most common phrases in the input.
"""